{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import time\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_names():\n",
    "    program_url = \"https://ic2s2-2023.org/program\" # Program URL\n",
    "    chairs_url = \"https://ic2s2-2023.org/organization\" # Chairs URL\n",
    "    keynotes_url = \"https://ic2s2-2023.org/keynotes\" # Keynotes URL\n",
    "\n",
    "    # Scrape program page\n",
    "    program_response = requests.get(program_url)\n",
    "    program_soup = BeautifulSoup(program_response.text, 'html.parser')\n",
    "\n",
    "    # Extract and split names by comma and remove extra spaces\n",
    "    program_names = []\n",
    "    for tag in program_soup.find_all('i'):\n",
    "        names = tag.get_text(strip=True).split(',')  # Split by comma\n",
    "        clean_names = [name.strip('\" ').strip() for name in names]  # Remove extra spaces/quotes\n",
    "        program_names.extend(clean_names)  # Add to the final list\n",
    "\n",
    "    # Scrape chairs page\n",
    "    chairs_response = requests.get(chairs_url)\n",
    "    chairs_soup = BeautifulSoup(chairs_response.text, 'html.parser')\n",
    "    chairs_names = [a.get_text(strip=True) for a in chairs_soup.select(\"h3 a\")] # names are stored in <a> within <h3> tags\n",
    "\n",
    "    # Scape keynotes page\n",
    "    keynotes_response = requests.get(keynotes_url)\n",
    "    keynotes_soup = BeautifulSoup(keynotes_response.text, 'html.parser')\n",
    "    keynotes_names = [a.get_text(strip=True) for a in keynotes_soup.select(\"h3 a\")] # names are stored in <a> within <h3> tags again\n",
    "\n",
    "    # Returning names in dict \n",
    "    return {\n",
    "        \"program_names\": program_names,\n",
    "        \"chairs_names\": chairs_names,\n",
    "        \"keynotes_names\": keynotes_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = scrape_names() # Getting all names\n",
    "\n",
    "Elements_to_remove = [\"SAGE\", \"DIREC\", \"EPJ\", \"Esteban Moro (EPJkeynote)\"] # Found some errors in keynotes_names - we just remove them manually\n",
    "for element in Elements_to_remove:\n",
    "    names[\"keynotes_names\"].remove(element)\n",
    "\n",
    "names[\"keynotes_names\"].append(\"Esteban Moro\") # Add back Esteban Moro without the (EPJkeynote) part\n",
    "\n",
    "all_names = names[\"program_names\"] + names[\"chairs_names\"] + names[\"keynotes_names\"] # combining all names into a list\n",
    "\n",
    "all_names = list(set(all_names)) # This removes 578 duplicates\n",
    "\n",
    "# remove all names that contain the string Chair\n",
    "all_names = [name for name in all_names if \"Chair\" not in name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final list with names\n",
    "len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar names: [('Diogo Pachecho', 'Diogo Pacheco'), ('Stefan M. Herzog', 'Stefan Herzog'), ('David Rothschild', 'David M Rothschild'), ('Francesco Pierri', 'Francesco Barbieri'), ('Francesco Pierri', 'Francesco Silvestri'), ('Luca Verginer', 'luca verginer'), ('Shintaro Sakai', 'Shintaro Ueki'), ('Ziwen Chen', 'Zexun Chen'), ('Woo-sung Jung', 'Woo-Sung Jung'), ('Luis E C Rocha', 'Luis M Rocha'), ('David Rand', 'David G. Rand'), ('Katinka den Nijs', 'Katinka Den Nijs'), ('Anne C. Kroon', 'Anne Kroon'), ('Anne C. Kroon', 'Anne C Kroon'), ('Zoe K. Rahwan', 'Zoe Rahwan'), ('Kathyrn R Fair', 'Kathyrn Fair'), ('Yuan Zhang', 'Yiyan Zhang'), ('Rupert Tibor Kiddle', 'Rupert Kiddle'), ('Amy Smith', 'Abby Smith'), ('Pantelis Analytis', 'Pantelis P. Analytis'), ('Pantelis Analytis', 'Pantelis P Analytis'), ('Federico Barrera-Lemarchand', 'Federico Barrera Lemarchand'), ('Ana María Jaramillo', 'Ana Maria Jaramillo'), ('Qi Wang', 'Bin Wang'), ('Qi Wang', 'Hui Wang'), ('Fabio Carella', 'Fabio Carrella'), ('Leo Ferres', 'Leonardo Ferres'), ('Matthew R DeVerna', 'Matthew DeVerna'), ('Xiao Zhang', 'Xiajie Zhang'), ('Yiyan Zhang', 'Feiyang Zhang'), ('Fabian Baumann', 'Fabian Braesemann'), ('Manoel Ribeiro', 'Manoel Horta Ribeiro'), ('Márton Karsai', 'Marton Karsai'), ('Nicholas A Christakis', 'Nicholas Christakis'), ('Carlos Rodriguez-Sickert', 'Carlos Rodriguez'), ('Sonja M Schmer-Galunder', 'Sonja M Schmer Galunder'), ('Samuel Woolley', 'Samuel Wolken'), ('Samuel Wolken', 'Sam Wolken'), ('Michele Tizzani', 'Michele Tizzoni'), ('Bedoor AlShebli', 'Bedoor Alshebli'), ('Anne Kroon', 'Anne C Kroon'), ('Duncan J Watts', 'Duncan J. Watts'), ('Duncan J Watts', 'Duncan Watts'), ('Xinyi Wang', 'Xindi Wang'), ('Jose Javier Ramasco', 'José Javier Ramasco'), ('Carlson Moses Büth', 'Carlson Büth'), ('Lisette Espin-Noboa', 'Lisette Espin Noboa'), ('Pantelis P. Analytis', 'Pantelis P Analytis'), ('Sanja Šćepanović', 'Sanja Scepanovic'), ('Sebastian Stier', 'Sebastian Steffen'), ('Ke Zhou', 'Keyu Zhu'), ('Duncan J. Watts', 'Duncan Watts'), ('Mariano Gaston Beiro', 'Mariano Gastón Beiró'), ('Huiyi Lv', 'Huiyi Lyu'), ('Scott A Hale', 'Scott A. Hale'), ('Babak Heydari', 'Sara Heydari'), ('Alessandro Flammini', 'Alessandro Flamini'), ('Marcos A. Oliveira', 'Marcos Oliveira'), ('Laura Alessandretti', 'Laura Maria Alessandretti')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# < ---- > \n",
    "# Method to find highly similar names\n",
    "# Should work with defualt python libs\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def are_almost_similar(s1, s2, threshold=0.8):\n",
    "    return SequenceMatcher(None, s1, s2).ratio() >= threshold\n",
    "\n",
    "def find_similar_names(all_names, threshold=0.8):\n",
    "    similar_pairs = []\n",
    "    for i in range(len(all_names)):\n",
    "        for j in range(i + 1, len(all_names)):\n",
    "            if are_almost_similar(all_names[i], all_names[j], threshold):\n",
    "                similar_pairs.append((all_names[i], all_names[j]))\n",
    "    return similar_pairs\n",
    "\n",
    "# < ---- >\n",
    "\n",
    "# Find and print similar names\n",
    "similar_names = find_similar_names(all_names, threshold=0.8)\n",
    "print(f\"similar names: {similar_names}\")\n",
    "\n",
    "# In most cases the similar names are just the same name, but including the middle name or initial.\n",
    "# Lets keep the longest entry and remove the shorter one\n",
    "# Identify names to remove\n",
    "names_to_remove = {name2 if len(name1) > len(name2) else name1 for name1, name2 in similar_names}\n",
    "\n",
    "# Remove names\n",
    "all_names = [name for name in all_names if name not in names_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n"
     ]
    }
   ],
   "source": [
    "# Final list of names\n",
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scarping Names Explained\n",
    "\n",
    "### 1. Inspecting URLs\n",
    "First, I inspected all the URLs to determine the structure in which the names were stored.\n",
    "\n",
    "### 2. Collecting and Printing Names\n",
    "Then, I collected all the names and printed them from each URL to verify that the extraction worked correctly.\n",
    "\n",
    "### 3. Handling Errors\n",
    "During this process, I found some errors in `keynotes_names`, but only **four names** were incorrect, so I removed them manually.\n",
    "\n",
    "### 4. Removing Duplicates\n",
    "Next, I used Python’s `set()` function to eliminate duplicates.\n",
    "\n",
    "### 5. Finding Similar Names\n",
    "I used a method to identify **slightly similar** names. After finding some matches, I decided to remove the shortest name of similair pairs. It was almost just a matter of includeing middlename / initials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1444 [00:00<08:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Julian Polenz] ❌ Author not found (Time: 0.3329s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1444 [00:05<1:20:29,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Allison Koenecke] ✅ Found author in 3.3577s, retrieved works in 2.1014s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1444 [00:07<1:00:46,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jisu Kim] ✅ Found author in 0.5337s, retrieved works in 1.0212s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1444 [00:09<54:27,  2.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nikolitsa Grigoropoulou] ✅ Found author in 0.3415s, retrieved works in 1.5242s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1444 [00:10<48:09,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vanessa Cheung] ✅ Found author in 0.6142s, retrieved works in 0.9294s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1444 [00:16<1:16:34,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kunihiro Miyazaki] ✅ Found author in 0.4435s, retrieved works in 5.0537s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1444 [00:17<1:04:30,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sharon Kang] ❌ Author not found (Time: 1.6587s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1444 [00:21<1:13:26,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vrushabh Vilas Wadnere] ❌ Author not found (Time: 3.8709s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1444 [00:23<1:01:51,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kongmeng Liew] ✅ Found author in 0.6327s, retrieved works in 0.8910s\n",
      "Rate limit exceeded, sleeping for 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1444 [00:24<52:26,  2.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error searching for author Margaret Roberts: 429 Client Error: TOO MANY REQUESTS for url: https://api.openalex.org/authors?search=Margaret%20Roberts&filter=works_count:%3E5,works_count:%3C5000\n",
      "[Margaret Roberts] ❌ Author not found (Time: 1.3150s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1444 [00:28<1:05:07,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frederik Georg Hjorth] ✅ Found author in 0.5060s, retrieved works in 3.4246s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1444 [00:32<1:15:34,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diogo Pachecho] ❌ Author not found (Time: 4.1707s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1444 [00:37<1:29:52,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ralph Schroeder] ✅ Found author in 2.8935s, retrieved works in 2.2563s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1444 [00:39<1:15:45,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Leslie DeChurch] ✅ Found author in 0.4700s, retrieved works in 1.3438s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1444 [00:43<1:14:46,  3.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 121\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal authors not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_authors_not_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal unique articles found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_works)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(all_names)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Time to get works for each author\u001b[39;00m\n\u001b[1;32m     93\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 94\u001b[0m works \u001b[38;5;241m=\u001b[39m \u001b[43mget_works_by_author_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m works_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m works:\n",
      "Cell \u001b[0;32mIn[69], line 51\u001b[0m, in \u001b[0;36mget_works_by_author_id\u001b[0;34m(author_id)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     url \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?filter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilter_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&cursor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_cursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&per-page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&sort=publication_date:desc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded, sleeping for 1 seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import urllib\n",
    "# from bs4 import BeautifulSoup\n",
    "# from tqdm import tqdm\n",
    "# from difflib import SequenceMatcher\n",
    "# import time\n",
    "\n",
    "# # -------------------------\n",
    "# #       Converting names to IDs if the names exists in OpenAlex - otherwise skipping\n",
    "# # -------------------------\n",
    "\n",
    "# def convert_author_name_to_id(name):\n",
    "#     query = urllib.parse.quote(name)\n",
    "#     url = f\"https://api.openalex.org/authors?search={query}&filter=works_count:>5,works_count:<5000\"\n",
    "    \n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 429:\n",
    "#             print(\"Rate limit exceeded, sleeping for 1 seconds\")\n",
    "#             time.sleep(1)\n",
    "            \n",
    "#         response.raise_for_status()\n",
    "#     except requests.exceptions.HTTPError as e:\n",
    "#         print(f\"Error searching for author {name}: {e}\")\n",
    "#         return None\n",
    "\n",
    "#     data = response.json()\n",
    "#     if \"results\" not in data or len(data[\"results\"]) == 0:\n",
    "#         return None\n",
    "\n",
    "#     # Take the first result\n",
    "#     first_result = data[\"results\"][0]\n",
    "#     return first_result[\"id\"] \n",
    "\n",
    "# # -------------------------\n",
    "# #       Collecting all works done by the authors. We use pagination to get all articles if there is more than 200\n",
    "# # -------------------------\n",
    "\n",
    "# def get_works_by_author_id(author_id):\n",
    "#     base_url = \"https://api.openalex.org/works\"\n",
    "#     filter_str = f\"author.id:{author_id},cited_by_count:>10,authors_count:<10\"\n",
    "#     next_cursor = \"*\"\n",
    "#     page_size = 200\n",
    "#     all_works = []\n",
    "    \n",
    "#     while True:\n",
    "#         url = (f\"{base_url}?filter={filter_str}\"\n",
    "#                f\"&cursor={next_cursor}\"\n",
    "#                f\"&per-page={page_size}\"\n",
    "#                f\"&sort=publication_date:desc\")\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 429:\n",
    "#             print(\"Rate limit exceeded, sleeping for 1 seconds\")\n",
    "#             time.sleep(1)\n",
    "#             continue\n",
    "#         response.raise_for_status()\n",
    "\n",
    "#         data = response.json()\n",
    "\n",
    "#         # Collect all works from this page\n",
    "#         works_this_page = data[\"results\"]\n",
    "\n",
    "#         all_works.extend(works_this_page)\n",
    "\n",
    "#         # Pagination\n",
    "#         next_cursor = data[\"meta\"].get(\"next_cursor\")\n",
    "#         if not next_cursor:\n",
    "#             break\n",
    "\n",
    "#     return all_works\n",
    "\n",
    "# # -------------------------\n",
    "# #       Main Logic\n",
    "# # -------------------------\n",
    "# def main(all_names):\n",
    "#     authors = all_names\n",
    "#     unique_works = {}\n",
    "#     num_authors_not_found = 0\n",
    "\n",
    "#     for name in tqdm(authors):\n",
    "        \n",
    "#         # Time for convert_author_name_to_id\n",
    "#         start_time = time.perf_counter()\n",
    "#         author_id = convert_author_name_to_id(name)\n",
    "#         convert_time = time.perf_counter() - start_time\n",
    "\n",
    "#         if author_id is None:\n",
    "#             num_authors_not_found += 1\n",
    "#             print(f\"[{name}] ❌ Author not found (Time: {convert_time:.4f}s)\")\n",
    "#             continue\n",
    "\n",
    "#         # Time to get works for each author\n",
    "#         start_time = time.perf_counter()\n",
    "#         works = get_works_by_author_id(author_id)\n",
    "#         works_time = time.perf_counter() - start_time\n",
    "\n",
    "#         if works:\n",
    "#             for w in works:\n",
    "#                 work_id = w.get(\"id\")\n",
    "#                 if work_id not in unique_works:\n",
    "#                     unique_works[work_id] = {\n",
    "#                         \"id\": work_id,\n",
    "#                         \"publication_year\": w.get(\"publication_year\"),\n",
    "#                         \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "#                         \"author_ids\": [\n",
    "#                             auth[\"author\"][\"id\"]\n",
    "#                             for auth in w.get(\"authorships\", [])\n",
    "#                             if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "#                         ],\n",
    "#                         \"title\": w.get(\"title\"),\n",
    "#                         \"abstract_inverted_index\": w.get(\"abstract_inverted_index\"),\n",
    "#                     }\n",
    "\n",
    "#         # Print time meassurements\n",
    "#         print(f\"[{name}] ✅ Found author in {convert_time:.4f}s, retrieved works in {works_time:.4f}s\")\n",
    "\n",
    "#     print(f\"\\nTotal authors not found: {num_authors_not_found}\")\n",
    "#     print(f\"Total unique articles found: {len(unique_works)}\")\n",
    "\n",
    "\n",
    "# result = main(all_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[convert_author_name_to_id] Error searching for author 'Kongmeng Liew': 429 Client Error: TOO MANY REQUESTS for url: https://api.openalex.org/authors?search=Kongmeng%20Liew&filter=works_count:%3E5,works_count:%3C5000\n",
      "[convert_author_name_to_id] Error searching for author 'Margaret Roberts': 429 Client Error: TOO MANY REQUESTS for url: https://api.openalex.org/authors?search=Margaret%20Roberts&filter=works_count:%3E5,works_count:%3C5000\n",
      "[convert_author_name_to_id] Error searching for author 'Frederik Georg Hjorth': 429 Client Error: TOO MANY REQUESTS for url: https://api.openalex.org/authors?search=Frederik%20Georg%20Hjorth&filter=works_count:%3E5,works_count:%3C5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[convert_author_name_to_id] Error searching for author 'Gennaro Cordasco': 429 Client Error: TOO MANY REQUESTS for url: https://api.openalex.org/authors?search=Gennaro%20Cordasco&filter=works_count:%3E5,works_count:%3C5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[convert_author_name_to_id] Error searching for author 'Brooke Foucault Welles': 429 Client Error: TOO MANY REQUESTS for url: https://api.openalex.org/authors?search=Brooke%20Foucault%20Welles&filter=works_count:%3E5,works_count:%3C5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 134\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique works after filtering duplicates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_works)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_works\n\u001b[0;32m--> 134\u001b[0m final_works \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 104\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(all_names)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(all_names):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# STEP A: Convert names -> IDs in parallel\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     name_to_id_map \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_author_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     valid_author_ids \u001b[38;5;241m=\u001b[39m [aid \u001b[38;5;28;01mfor\u001b[39;00m aid \u001b[38;5;129;01min\u001b[39;00m name_to_id_map\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m aid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFound valid IDs for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_author_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m authors.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[81], line 33\u001b[0m, in \u001b[0;36mget_all_author_ids\u001b[0;34m(author_names, n_jobs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mConvert each author name to an OpenAlex author ID in parallel.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mReturns a dict {author_name: author_id or None}.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Use Joblib to parallelize:\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_author_name_to_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConverting Names to IDs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Combine names and results into a dictionary\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(author_names, results))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def convert_author_name_to_id(name):\n",
    "    \"\"\"\n",
    "    Return the first matching author ID from OpenAlex if they have 5-5000 works.\n",
    "    Returns None if no match or an error occurs.\n",
    "    \"\"\"\n",
    "    query = urllib.parse.quote(name)\n",
    "    url = f\"https://api.openalex.org/authors?search={query}&filter=works_count:>5,works_count:<5000\"\n",
    "\n",
    "    # Simple rate-limiting sleep (~10 requests/sec across all workers)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[convert_author_name_to_id] Error searching for author '{name}': {e}\")\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    if \"results\" not in data or len(data[\"results\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take the first match\n",
    "    first_result = data[\"results\"][0]\n",
    "    return first_result[\"id\"]  # e.g. \"https://openalex.org/A123456789\"\n",
    "\n",
    "def get_all_author_ids(author_names, n_jobs=2):\n",
    "    \"\"\"\n",
    "    Convert each author name to an OpenAlex author ID in parallel.\n",
    "    Returns a dict {author_name: author_id or None}.\n",
    "    \"\"\"\n",
    "    # Use Joblib to parallelize:\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(convert_author_name_to_id)(name) for name in tqdm(author_names, desc=\"Converting Names to IDs\")\n",
    "    )\n",
    "    # Combine names and results into a dictionary\n",
    "    return dict(zip(author_names, results))\n",
    "\n",
    "def get_works_for_author_batch(author_ids):\n",
    "    \"\"\"\n",
    "    Retrieve works for up to 25 author IDs in ONE call, filtering by:\n",
    "      - cited_by_count:>10\n",
    "      - authors_count:<10\n",
    "    Returns a list of all works for these authors combined.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    \n",
    "    # Build OR filter for all author IDs in this batch\n",
    "    # Example: author.id:A1|author.id:A2|...|author.id:A25\n",
    "    author_filter = \"|\".join([f\"author.id:{a}\" for a in author_ids])\n",
    "    filter_str = f\"{author_filter},cited_by_count:>10,authors_count:<10\"\n",
    "\n",
    "    all_works = []\n",
    "    cursor = \"*\"\n",
    "\n",
    "    while True:\n",
    "        url = (f\"{base_url}?filter={filter_str}\"\n",
    "               f\"&cursor={cursor}\"\n",
    "               f\"&per-page=200\"\n",
    "               f\"&sort=publication_date:desc\")\n",
    "        \n",
    "        # Simple rate-limiting\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[get_works_for_author_batch] Error fetching works for batch: {e}\")\n",
    "            return all_works  # Return what we have so far\n",
    "\n",
    "        data = response.json()\n",
    "        all_works.extend(data.get(\"results\", []))\n",
    "\n",
    "        next_cursor = data[\"meta\"].get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break\n",
    "        cursor = next_cursor\n",
    "\n",
    "    return all_works\n",
    "\n",
    "\n",
    "def get_works_in_parallel(author_ids, n_jobs=1, batch_size=25):\n",
    "    \"\"\"\n",
    "    Break the author_ids list into chunks of `batch_size`,\n",
    "    fetch works for each chunk in parallel, and combine into one list.\n",
    "    \"\"\"\n",
    "    # Split IDs into chunks of up to 25\n",
    "    chunks = [author_ids[i : i + batch_size] for i in range(0, len(author_ids), batch_size)]\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_works_for_author_batch)(chunk) for chunk in tqdm(chunks, desc=\"Fetching Works in Batches\")\n",
    "    )\n",
    "    \n",
    "    # Flatten the list of lists\n",
    "    all_works = []\n",
    "    for works_list in results:\n",
    "        all_works.extend(works_list)\n",
    "    \n",
    "    return all_works\n",
    "\n",
    "def main(all_names):\n",
    "    # STEP A: Convert names -> IDs in parallel\n",
    "    name_to_id_map = get_all_author_ids(all_names, n_jobs=4)\n",
    "    valid_author_ids = [aid for aid in name_to_id_map.values() if aid is not None]\n",
    "    print(f\"\\nFound valid IDs for {len(valid_author_ids)} out of {len(all_names)} authors.\")\n",
    "\n",
    "    # STEP B: Fetch works in bulk (up to 25 authors per request)\n",
    "    all_works = get_works_in_parallel(valid_author_ids, n_jobs=4, batch_size=25)\n",
    "    print(f\"\\nTotal works fetched (with duplicates): {len(all_works)}\")\n",
    "\n",
    "    # STEP C: Deduplicate\n",
    "    unique_works = {}\n",
    "    for w in all_works:\n",
    "        work_id = w.get(\"id\")\n",
    "        if work_id not in unique_works:\n",
    "            unique_works[work_id] = {\n",
    "                \"id\": work_id,\n",
    "                \"publication_year\": w.get(\"publication_year\"),\n",
    "                \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "                \"author_ids\": [\n",
    "                    auth[\"author\"][\"id\"]\n",
    "                    for auth in w.get(\"authorships\", [])\n",
    "                    if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "                ],\n",
    "                \"title\": w.get(\"title\"),\n",
    "                \"abstract_inverted_index\": w.get(\"abstract_inverted_index\"),\n",
    "            }\n",
    "\n",
    "    print(f\"Unique works after filtering duplicates: {len(unique_works)}\")\n",
    "    return unique_works\n",
    "\n",
    "\n",
    "final_works = main(all_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "DEBUG_MODE = True  # Set to False if you don't want detailed debug prints\n",
    "\n",
    "def debug_print(msg):\n",
    "    \"\"\"Helper to conditionally print debug messages.\"\"\"\n",
    "    if DEBUG_MODE:\n",
    "        print(msg)\n",
    "\n",
    "def convert_author_name_to_id(name):\n",
    "    \"\"\"\n",
    "    Return the first matching author ID from OpenAlex if they have 5-5000 works.\n",
    "    Returns None if no match or an error occurs.\n",
    "    \"\"\"\n",
    "    query = urllib.parse.quote(name)\n",
    "    url = f\"https://api.openalex.org/authors?search={query}&filter=works_count:>5,works_count:<5000\"\n",
    "\n",
    "    # Simple rate-limiting sleep (~10 requests/sec across all workers)\n",
    "    # If you still get 429 errors, increase this sleep or reduce n_jobs.\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    debug_print(f\"[convert_author_name_to_id] GET {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 429:\n",
    "            debug_print(\"[convert_author_name_to_id] 429 Too Many Requests. Consider backoff or slower requests.\")\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[convert_author_name_to_id] Error searching for author '{name}': {e}\")\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    if \"results\" not in data or len(data[\"results\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take the first match\n",
    "    first_result = data[\"results\"][0]\n",
    "    return first_result[\"id\"]\n",
    "\n",
    "def get_all_author_ids(author_names, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Convert each author name to an OpenAlex author ID in parallel.\n",
    "    Returns a dict {author_name: author_id or None}.\n",
    "    \"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(convert_author_name_to_id)(name)\n",
    "        for name in tqdm(author_names, desc=\"Converting Names to IDs\")\n",
    "    )\n",
    "    return dict(zip(author_names, results))\n",
    "\n",
    "def get_works_for_author_batch(author_ids, max_retries=5):\n",
    "    \"\"\"\n",
    "    Retrieve works for up to 25 author IDs in ONE call, filtering by:\n",
    "      - cited_by_count:>10\n",
    "      - authors_count:<10\n",
    "    Returns a list of all works for these authors combined.\n",
    "    \n",
    "    Includes simple exponential backoff if a 429 error occurs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    author_filter = \"|\".join([f\"author.id:{a}\" for a in author_ids])\n",
    "    filter_str = f\"{author_filter},cited_by_count:>10,authors_count:<10\"\n",
    "\n",
    "    all_works = []\n",
    "    cursor = \"*\"\n",
    "    \n",
    "    attempts = 0\n",
    "\n",
    "    while True:\n",
    "        url = (\n",
    "            f\"{base_url}?filter={filter_str}\"\n",
    "            f\"&cursor={cursor}\"\n",
    "            f\"&per-page=200\"\n",
    "            f\"&sort=publication_date:desc\"\n",
    "        )\n",
    "\n",
    "        debug_print(f\"[get_works_for_author_batch] GET {url}\")\n",
    "\n",
    "        # Simple rate-limiting\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        response = None\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 429:\n",
    "                    # Exponential backoff\n",
    "                    wait_time = 2 ** retry  # 1,2,4,8,...\n",
    "                    print(f\"[get_works_for_author_batch] 429 Too Many Requests. Backing off for {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue  # Retry the request\n",
    "                # If not 429, then raise if error (400-599 except 429 handled above)\n",
    "                response.raise_for_status()\n",
    "                break  # If success, break the retry loop\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"[get_works_for_author_batch] Error: {e} (retry {retry + 1}/{max_retries})\")\n",
    "                # Try again up to max_retries\n",
    "                time.sleep(2 ** retry)\n",
    "        else:\n",
    "            # If we exit the for-loop without breaking, all retries failed\n",
    "            debug_print(\"[get_works_for_author_batch] Max retries hit. Returning what we have so far.\")\n",
    "            return all_works\n",
    "\n",
    "        if response is None:\n",
    "            # No response at all, return what we have\n",
    "            return all_works\n",
    "\n",
    "        data = response.json()\n",
    "        page_results = data.get(\"results\", [])\n",
    "        all_works.extend(page_results)\n",
    "\n",
    "        debug_print(f\"  - Retrieved {len(page_results)} works this page; total so far: {len(all_works)}\")\n",
    "\n",
    "        next_cursor = data[\"meta\"].get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break  # No more pages\n",
    "        cursor = next_cursor\n",
    "\n",
    "    return all_works\n",
    "\n",
    "def get_works_in_parallel(author_ids, n_jobs=4, batch_size=25):\n",
    "    \"\"\"\n",
    "    Break the author_ids list into chunks of `batch_size`,\n",
    "    fetch works for each chunk in parallel, and combine into one list.\n",
    "    \"\"\"\n",
    "    chunks = [author_ids[i : i + batch_size] for i in range(0, len(author_ids), batch_size)]\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_works_for_author_batch)(chunk) \n",
    "        for chunk in tqdm(chunks, desc=\"Fetching Works in Batches\")\n",
    "    )\n",
    "    \n",
    "    # Flatten\n",
    "    all_works = []\n",
    "    for works_list in results:\n",
    "        all_works.extend(works_list)\n",
    "    return all_works\n",
    "\n",
    "def main(all_names):\n",
    "    # STEP A: Convert names -> IDs in parallel\n",
    "    name_to_id_map = get_all_author_ids(all_names, n_jobs=4)\n",
    "    valid_author_ids = [aid for aid in name_to_id_map.values() if aid is not None]\n",
    "    print(f\"\\nFound valid IDs for {len(valid_author_ids)} out of {len(all_names)} authors.\")\n",
    "\n",
    "    if not valid_author_ids:\n",
    "        print(\"No valid author IDs found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # STEP B: Fetch works in bulk (up to 25 authors per request)\n",
    "    all_works = get_works_in_parallel(valid_author_ids, n_jobs=4, batch_size=25)\n",
    "    print(f\"\\nTotal works fetched (with duplicates): {len(all_works)}\")\n",
    "\n",
    "    # STEP C: Deduplicate\n",
    "    unique_works = {}\n",
    "    for w in all_works:\n",
    "        work_id = w.get(\"id\")\n",
    "        if work_id not in unique_works:\n",
    "            unique_works[work_id] = {\n",
    "                \"id\": work_id,\n",
    "                \"publication_year\": w.get(\"publication_year\"),\n",
    "                \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "                \"author_ids\": [\n",
    "                    auth[\"author\"][\"id\"]\n",
    "                    for auth in w.get(\"authorships\", [])\n",
    "                    if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "                ],\n",
    "                \"title\": w.get(\"title\"),\n",
    "                \"abstract_inverted_index\": w.get(\"abstract_inverted_index\"),\n",
    "            }\n",
    "\n",
    "    print(f\"Unique works after filtering duplicates: {len(unique_works)}\")\n",
    "    return unique_works\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_names = [\n",
    "        \"Albert Einstein\", \n",
    "        \"Marie Curie\", \n",
    "        \"Allison Koenecke\", \n",
    "        \"Julian Polenz\", \n",
    "        \"Jisu Kim\"\n",
    "    ]\n",
    "    final_works = main(test_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#       Using the data from unique_works to create two dataframes - one for papers and one for abstracts\n",
    "# -------------------------\n",
    "papers_data = []\n",
    "abstracts_data = []\n",
    "\n",
    "for work_id, work in unique_works.items():\n",
    "    papers_data.append({\n",
    "        \"id\": work[\"id\"],\n",
    "        \"publication_year\": work[\"publication_year\"],\n",
    "        \"cited_by_count\": work[\"cited_by_count\"],\n",
    "        \"author_ids\": \",\".join(work[\"author_ids\"])  \n",
    "    })\n",
    "    abstracts_data.append({\n",
    "        \"id\": work[\"id\"],\n",
    "        \"title\": work[\"title\"],\n",
    "        \"abstract_inverted_index\": work[\"abstract_inverted_index\"]\n",
    "    })\n",
    "\n",
    "df_papers = pd.DataFrame(papers_data)\n",
    "df_abstracts = pd.DataFrame(abstracts_data)\n",
    "\n",
    "df_papers.to_csv(\"IC2S2_papers.csv\", index=False)\n",
    "df_abstracts.to_csv(\"IC2S2_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 4</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12016/12016 [00:00<00:00, 203637.10it/s]\n",
      "100%|██████████| 85941/85941 [00:00<00:00, 6062067.88it/s]\n",
      "100%|██████████| 12016/12016 [00:00<00:00, 43387.99it/s]\n",
      "100%|██████████| 22183/22183 [00:00<00:00, 1171964.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph saved successfully as 'IC2S2_coauthorship_network.json' 🎉\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "df_papers = pd.read_csv(\"IC2S2_papers.csv\")  # Load Papers Dataset\n",
    "\n",
    "# Create NetworkX Graph\n",
    "G = nx.Graph()\n",
    "collaboration_counts = Counter()\n",
    "\n",
    "\n",
    "for author_str in tqdm(df_papers[\"author_ids\"]): # loop over each paper\n",
    "    authors = author_str.split(\",\") # split the authors (seperated by ,)\n",
    "\n",
    "    for author1, author2 in itertools.combinations(authors, 2): # loop over each combination of authors\n",
    "        pair = tuple(sorted([author1, author2])) # sort the pair of authors to avoid duplicates\n",
    "        collaboration_counts[pair] += 1 # count the number of collaborations between the pair of authors\n",
    "\n",
    "G.add_weighted_edges_from([(a1, a2, count) for (a1, a2), count in tqdm(collaboration_counts.items())])\n",
    "\n",
    "\n",
    "\n",
    "# Compute citation counts and first publication year for each author\n",
    "author_citation_counts = Counter()\n",
    "author_first_publication = {}\n",
    "\n",
    "for _, row in tqdm(df_papers.iterrows(), total=len(df_papers)):\n",
    "    authors = row[\"author_ids\"].split(\",\")\n",
    "    cited_by = row[\"cited_by_count\"] # Number of citations for given paper\n",
    "    pub_year = row[\"publication_year\"] # Publication year of given paper\n",
    "\n",
    "    for author in authors:\n",
    "        author_citation_counts[author] += cited_by  # Sum up citations for each author\n",
    "        if author not in author_first_publication or pub_year < author_first_publication[author]: # Keep earliest year \n",
    "            author_first_publication[author] = pub_year\n",
    "\n",
    "# Add info to nodes\n",
    "for node in tqdm(G.nodes()):\n",
    "    G.nodes[node][\"display_name\"] = \"Unknown\"\n",
    "    G.nodes[node][\"country\"] = \"Unknown\"\n",
    "    G.nodes[node][\"total_citations\"] = author_citation_counts.get(node, 0)\n",
    "    G.nodes[node][\"first_publication_year\"] = author_first_publication.get(node, None)\n",
    "\n",
    "\n",
    "graph_data = nx.node_link_data(G)\n",
    "\n",
    "with open(\"IC2S2_coauthorship_network.json\", \"w\") as json_file:\n",
    "    json.dump(graph_data, json_file, indent=4)\n",
    "\n",
    "print(\"\\nGraph saved successfully as 'IC2S2_coauthorship_network.json' 🎉\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
