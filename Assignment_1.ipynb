{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_names():\n",
    "    program_url = \"https://ic2s2-2023.org/program\" # Program URL\n",
    "    chairs_url = \"https://ic2s2-2023.org/organization\" # Chairs URL\n",
    "    keynotes_url = \"https://ic2s2-2023.org/keynotes\" # Keynotes URL\n",
    "\n",
    "    # Scrape program page\n",
    "    program_response = requests.get(program_url)\n",
    "    program_soup = BeautifulSoup(program_response.text, 'html.parser')\n",
    "\n",
    "    # Extract and split names by comma and remove extra spaces\n",
    "    program_names = []\n",
    "    for tag in program_soup.find_all('i'):\n",
    "        names = tag.get_text(strip=True).split(',')  # Split by comma\n",
    "        clean_names = [name.strip('\" ').strip() for name in names]  # Remove extra spaces/quotes\n",
    "        program_names.extend(clean_names)  # Add to the final list\n",
    "\n",
    "    # Scrape chairs page\n",
    "    chairs_response = requests.get(chairs_url)\n",
    "    chairs_soup = BeautifulSoup(chairs_response.text, 'html.parser')\n",
    "    chairs_names = [a.get_text(strip=True) for a in chairs_soup.select(\"h3 a\")] # names are stored in <a> within <h3> tags\n",
    "\n",
    "    # Scape keynotes page\n",
    "    keynotes_response = requests.get(keynotes_url)\n",
    "    keynotes_soup = BeautifulSoup(keynotes_response.text, 'html.parser')\n",
    "    keynotes_names = [a.get_text(strip=True) for a in keynotes_soup.select(\"h3 a\")] # names are stored in <a> within <h3> tags again\n",
    "\n",
    "    # Returning names in dict \n",
    "    return {\n",
    "        \"program_names\": program_names,\n",
    "        \"chairs_names\": chairs_names,\n",
    "        \"keynotes_names\": keynotes_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = scrape_names() # Getting all names\n",
    "\n",
    "Elements_to_remove = [\"SAGE\", \"DIREC\", \"EPJ\", \"Esteban Moro (EPJkeynote)\"] # Found some errors in keynotes_names - we just remove them manually\n",
    "for element in Elements_to_remove:\n",
    "    names[\"keynotes_names\"].remove(element)\n",
    "\n",
    "names[\"keynotes_names\"].append(\"Esteban Moro\") # Add back Esteban Moro without the (EPJkeynote) part\n",
    "\n",
    "all_names = names[\"program_names\"] + names[\"chairs_names\"] + names[\"keynotes_names\"] # combining all names into a list\n",
    "\n",
    "all_names = list(set(all_names)) # This removes 578 duplicates\n",
    "\n",
    "# remove all names that contain the string Chair\n",
    "all_names = [name for name in all_names if \"Chair\" not in name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final list with names\n",
    "len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar names: [('Yiyan Zhang', 'Feiyang Zhang'), ('Yiyan Zhang', 'Yuan Zhang'), ('Michele Tizzani', 'Michele Tizzoni'), ('Keyu Zhu', 'Ke Zhou'), ('Leonardo Ferres', 'Leo Ferres'), ('Rupert Kiddle', 'Rupert Tibor Kiddle'), ('Stefan M. Herzog', 'Stefan Herzog'), ('Francesco Barbieri', 'Francesco Pierri'), ('David M Rothschild', 'David Rothschild'), ('Bin Wang', 'Qi Wang'), ('Duncan J. Watts', 'Duncan J Watts'), ('Duncan J. Watts', 'Duncan Watts'), ('Fabio Carella', 'Fabio Carrella'), ('Shintaro Sakai', 'Shintaro Ueki'), ('Francesco Pierri', 'Francesco Silvestri'), ('Scott A. Hale', 'Scott A Hale'), ('Woo-sung Jung', 'Woo-Sung Jung'), ('Sonja M Schmer Galunder', 'Sonja M Schmer-Galunder'), ('Xindi Wang', 'Xinyi Wang'), ('Huiyi Lyu', 'Huiyi Lv'), ('MÃ¡rton Karsai', 'Marton Karsai'), ('Laura Alessandretti', 'Laura Maria Alessandretti'), ('Hui Wang', 'Qi Wang'), ('Jose Javier Ramasco', 'JosÃ© Javier Ramasco'), ('Sanja Å Ä‡epanoviÄ‡', 'Sanja Scepanovic'), ('luca verginer', 'Luca Verginer'), ('Bedoor AlShebli', 'Bedoor Alshebli'), ('Katinka Den Nijs', 'Katinka den Nijs'), ('Ana Maria Jaramillo', 'Ana MarÃ­a Jaramillo'), ('Xiao Zhang', 'Xiajie Zhang'), ('Zoe K. Rahwan', 'Zoe Rahwan'), ('Alessandro Flammini', 'Alessandro Flamini'), ('Babak Heydari', 'Sara Heydari'), ('Anne Kroon', 'Anne C. Kroon'), ('Anne Kroon', 'Anne C Kroon'), ('Diogo Pacheco', 'Diogo Pachecho'), ('David G. Rand', 'David Rand'), ('Pantelis Analytis', 'Pantelis P. Analytis'), ('Pantelis Analytis', 'Pantelis P Analytis'), ('Sebastian Steffen', 'Sebastian Stier'), ('Mariano GastÃ³n BeirÃ³', 'Mariano Gaston Beiro'), ('Manoel Horta Ribeiro', 'Manoel Ribeiro'), ('Luis E C Rocha', 'Luis M Rocha'), ('Samuel Wolken', 'Sam Wolken'), ('Samuel Wolken', 'Samuel Woolley'), ('Fabian Braesemann', 'Fabian Baumann'), ('Anne C. Kroon', 'Anne C Kroon'), ('Abby Smith', 'Amy Smith'), ('Pantelis P. Analytis', 'Pantelis P Analytis'), ('Carlos Rodriguez', 'Carlos Rodriguez-Sickert'), ('Federico Barrera Lemarchand', 'Federico Barrera-Lemarchand'), ('Lisette Espin-Noboa', 'Lisette Espin Noboa'), ('Nicholas Christakis', 'Nicholas A Christakis'), ('Matthew DeVerna', 'Matthew R DeVerna'), ('Ziwen Chen', 'Zexun Chen'), ('Carlson Moses BÃ¼th', 'Carlson BÃ¼th'), ('Marcos Oliveira', 'Marcos A. Oliveira'), ('Duncan J Watts', 'Duncan Watts'), ('Kathyrn R Fair', 'Kathyrn Fair')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# < ---- > \n",
    "# Method to find highly similar names\n",
    "# Should work with defualt python libs\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def are_almost_similar(s1, s2, threshold=0.8):\n",
    "    return SequenceMatcher(None, s1, s2).ratio() >= threshold\n",
    "\n",
    "def find_similar_names(all_names, threshold=0.8):\n",
    "    similar_pairs = []\n",
    "    for i in range(len(all_names)):\n",
    "        for j in range(i + 1, len(all_names)):\n",
    "            if are_almost_similar(all_names[i], all_names[j], threshold):\n",
    "                similar_pairs.append((all_names[i], all_names[j]))\n",
    "    return similar_pairs\n",
    "\n",
    "# < ---- >\n",
    "\n",
    "# Find and print similar names\n",
    "similar_names = find_similar_names(all_names, threshold=0.8)\n",
    "print(f\"similar names: {similar_names}\")\n",
    "\n",
    "# In most cases the similar names are just the same name, but including the middle name or initial.\n",
    "# Lets keep the longest entry and remove the shorter one\n",
    "# Identify names to remove\n",
    "names_to_remove = {name2 if len(name1) > len(name2) else name1 for name1, name2 in similar_names}\n",
    "\n",
    "# Remove names\n",
    "all_names = [name for name in all_names if name not in names_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n"
     ]
    }
   ],
   "source": [
    "# Final list of names\n",
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scarping Names Explained\n",
    "\n",
    "### 1. Inspecting URLs\n",
    "First, I inspected all the URLs to determine the structure in which the names were stored.\n",
    "\n",
    "### 2. Collecting and Printing Names\n",
    "Then, I collected all the names and printed them from each URL to verify that the extraction worked correctly.\n",
    "\n",
    "### 3. Handling Errors\n",
    "During this process, I found some errors in `keynotes_names`, but only **four names** were incorrect, so I removed them manually.\n",
    "\n",
    "### 4. Removing Duplicates\n",
    "Next, I used Pythonâ€™s `set()` function to eliminate duplicates.\n",
    "\n",
    "### 5. Finding Similar Names\n",
    "I used a method to identify **slightly similar** names. After finding some matches, I decided to remove the shortest name of similair pairs. It was almost just a matter of includeing middlename / initials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pros and Cons of Custom-Made Data (Centola) vs. Ready-Made Data (Nicolaides)\n",
    "\n",
    "Centolaâ€™s study used custom-made data, which allowed him to control the structure of social networks. By creating artificial networks, he could adjust factors like how connected people were and how groups were formed. This control made it easier to see cause-and-effect relationships in behavior spread, making the results more reliable within the experiment. However, because the study took place in a simulated setting, it may not fully reflect how people behave in real life, limiting how well the findings apply outside the lab.\n",
    "\n",
    "Nicolaidesâ€™s study, on the other hand, used real-world data to observe how behaviors spread naturally in existing social networks. This makes the findings more realistic since they reflect actual human interactions. However, because the study was observational, there was less control over external factors. This means itâ€™s harder to prove that one specific factor caused the behavior to spread, as other influences might have played a role.\n",
    "\n",
    "### 2. Influence on Interpretation of Results\n",
    "\n",
    "The differences in how the two studies were conducted affect how we interpret their results. Centola used custom-made data and controlled network structures, which makes it easier to prove cause and effect in behavior spread. This is useful for building theories about how social influence works. However, since the study was done in an artificial setting, the results might not fully apply to real life.\n",
    "\n",
    "Nicolaides, on the other hand, studied real-world data, showing how behaviors actually spread in natural social networks. This makes the findings more realistic, but because the study wasnâ€™t controlled, itâ€™s harder to say for sure what caused the behaviors to spread. Other factors, like personal differences or outside influences, might have played a role. Overall, Centolaâ€™s study helps explain the theory behind social influence, while Nicolaidesâ€™s study shows how it works in practice, making them complementary to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting names to IDs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1444/1444 [02:44<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total authors not found: 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving works in bulk: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:44<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique articles found: 43737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "#       Rate Limiter\n",
    "# -------------------------\n",
    "class RateLimiter:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate  # requests per second\n",
    "        self.min_interval = 1.0 / rate\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_time = 0.0\n",
    "\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = time.perf_counter()\n",
    "            elapsed = now - self.last_time\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_time = time.perf_counter()\n",
    "\n",
    "# Global rate limiter instance (10 requests per second)\n",
    "rate_limiter = RateLimiter(10)\n",
    "\n",
    "# -------------------------\n",
    "#  Convert names to IDs\n",
    "# -------------------------\n",
    "def convert_author_name_to_id(name):\n",
    "    query = urllib.parse.quote(name)\n",
    "    url = f\"https://api.openalex.org/authors?search={query}&filter=works_count:>5,works_count:<5000\"\n",
    "    try:\n",
    "        rate_limiter.wait()\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 429: # if this is trickered, we wait for 1 second and wait on rate limiter\n",
    "            time.sleep(1)\n",
    "            rate_limiter.wait()\n",
    "            response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e: # other errors, then we skip\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    if \"results\" not in data or len(data[\"results\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take the first result\n",
    "    first_result = data[\"results\"][0]\n",
    "    return first_result[\"id\"]\n",
    "\n",
    "# -------------------------\n",
    "#  Get works by author IDs\n",
    "# -------------------------\n",
    "def get_works_by_authors_ids_bulk(author_ids):\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    authors_filter = \"author.id:\" + \"|\".join(author_ids)\n",
    "    filter_str = f\"{authors_filter},cited_by_count:>10,authors_count:<10\"\n",
    "    next_cursor = \"*\"\n",
    "    page_size = 200\n",
    "    all_works = []\n",
    "\n",
    "    while True:\n",
    "        url = (f\"{base_url}?filter={filter_str}\"\n",
    "               f\"&cursor={next_cursor}\"\n",
    "               f\"&per-page={page_size}\"\n",
    "               f\"&sort=publication_date:desc\")\n",
    "        try:\n",
    "            rate_limiter.wait()\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 429: # if this is trickered, we wait for 1 second\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        works_this_page = data.get(\"results\", [])\n",
    "        all_works.extend(works_this_page)\n",
    "        next_cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break\n",
    "\n",
    "    return all_works\n",
    "\n",
    "# -------------------------\n",
    "#       Main Logic\n",
    "# -------------------------\n",
    "\n",
    "# Convert all names to author IDs in parallel\n",
    "conversion_results = Parallel(n_jobs=10, backend=\"threading\")(\n",
    "    delayed(convert_author_name_to_id)(name) for name in tqdm(all_names, desc=\"Converting names to IDs\")\n",
    ")\n",
    "\n",
    "name_id_map = {name: author_id for name, author_id in zip(all_names, conversion_results) if author_id is not None}\n",
    "num_authors_not_found = len(all_names) - len(name_id_map)\n",
    "print(f\"\\nTotal authors not found: {num_authors_not_found}\")\n",
    "\n",
    "    \n",
    "author_ids = list(name_id_map.values()) # list containing all found author IDs\n",
    "\n",
    "\n",
    "def chunks(lst, n): # Splitting the list of author IDs into chunks of 25\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "author_id_chunks = list(chunks(author_ids, 25)) # calling the func\n",
    "\n",
    "# Get all works by author IDs in parallel\n",
    "works_lists = Parallel(n_jobs=10, backend=\"threading\")(\n",
    "    delayed(get_works_by_authors_ids_bulk)(chunk) for chunk in tqdm(author_id_chunks, desc=\"Retrieving works in bulk\")\n",
    ")\n",
    "\n",
    "# we collect only data for unique works\n",
    "all_works = [work for sublist in works_lists for work in sublist]\n",
    "unique_works = {}\n",
    "for w in all_works:\n",
    "    work_id = w.get(\"id\")\n",
    "    if work_id not in unique_works:\n",
    "        unique_works[work_id] = {\n",
    "            \"id\": work_id,\n",
    "            \"publication_year\": w.get(\"publication_year\"),\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "            \"author_ids\": [\n",
    "                auth[\"author\"][\"id\"]\n",
    "                for auth in w.get(\"authorships\", [])\n",
    "                if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "            ],\n",
    "            \"title\": w.get(\"title\"),\n",
    "            \"abstract_inverted_index\": w.get(\"abstract_inverted_index\"),\n",
    "        }\n",
    "\n",
    "print(f\"\\nTotal unique articles found: {len(unique_works)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#       Using the data from unique_works to create two dataframes - one for papers and one for abstracts\n",
    "# -------------------------\n",
    "papers_data = []\n",
    "abstracts_data = []\n",
    "\n",
    "for work_id, work in unique_works.items():\n",
    "    papers_data.append({\n",
    "        \"id\": work[\"id\"],\n",
    "        \"publication_year\": work[\"publication_year\"],\n",
    "        \"cited_by_count\": work[\"cited_by_count\"],\n",
    "        \"author_ids\": \",\".join(work[\"author_ids\"])  \n",
    "    })\n",
    "    abstracts_data.append({\n",
    "        \"id\": work[\"id\"],\n",
    "        \"title\": work[\"title\"],\n",
    "        \"abstract_inverted_index\": work[\"abstract_inverted_index\"]\n",
    "    })\n",
    "\n",
    "df_papers = pd.DataFrame(papers_data)\n",
    "df_abstracts = pd.DataFrame(abstracts_data)\n",
    "\n",
    "df_papers.to_csv(\"IC2S2_papers.csv\", index=False)\n",
    "df_abstracts.to_csv(\"IC2S2_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Research Articles using the OpenAlex API Explained\n",
    "\n",
    "### 1. Dataset Summary\n",
    "The IC2S2 papers dataset contains 43,737 works. These works were authored or co-authored by 1,179 (1444 - 265 not found) unique researchers.\n",
    "\n",
    "### 2. Efficiency in Code\n",
    "To improve efficiency, I implemented parallel processing using Joblibâ€™s Parallel to fetch author IDs and works concurrently, significantly reducing runtime. Additionally, I batched API requests, retrieving works for up to 25 authors at once, minimizing the number of queries sent. A rate limiter ensured compliance with OpenAlexâ€™s 10-requests-per-second limit, preventing failures due to excessive requests.\n",
    "\n",
    "### 3. Filtering Criteria and Dataset Relevance\n",
    "The dataset filters authors with at least 5 but fewer than 5,000 works, ensuring they are active but not overly prolific, which could introduce bias. Works are included only if they have at least 10 citations (indicating impact) and fewer than 10 authors (favoring individual contributions over large collaborations). These filters enhance dataset relevance by focusing on influential yet manageable research outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 4</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43737/43737 [00:00<00:00, 219786.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 274797/274797 [00:00<00:00, 5360847.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43737/43737 [00:00<00:00, 52641.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69158/69158 [00:00<00:00, 1088433.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph saved successfully as 'IC2S2_coauthorship_network.json' ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "df_papers = pd.read_csv(\"IC2S2_papers.csv\")  # Load Papers Dataset\n",
    "\n",
    "# Create NetworkX Graph\n",
    "G = nx.Graph()\n",
    "collaboration_counts = Counter()\n",
    "\n",
    "\n",
    "for author_str in tqdm(df_papers[\"author_ids\"]): # loop over each paper\n",
    "    authors = author_str.split(\",\") # split the authors (seperated by ,)\n",
    "\n",
    "    for author1, author2 in itertools.combinations(authors, 2): # loop over each combination of authors\n",
    "        pair = tuple(sorted([author1, author2])) # sort the pair of authors to avoid duplicates\n",
    "        collaboration_counts[pair] += 1 # count the number of collaborations between the pair of authors\n",
    "\n",
    "G.add_weighted_edges_from([(a1, a2, count) for (a1, a2), count in tqdm(collaboration_counts.items())])\n",
    "\n",
    "\n",
    "\n",
    "# Compute citation counts and first publication year for each author\n",
    "author_citation_counts = Counter()\n",
    "author_first_publication = {}\n",
    "\n",
    "for _, row in tqdm(df_papers.iterrows(), total=len(df_papers)):\n",
    "    authors = row[\"author_ids\"].split(\",\")\n",
    "    cited_by = row[\"cited_by_count\"] # Number of citations for given paper\n",
    "    pub_year = row[\"publication_year\"] # Publication year of given paper\n",
    "\n",
    "    for author in authors:\n",
    "        author_citation_counts[author] += cited_by  # Sum up citations for each author\n",
    "        if author not in author_first_publication or pub_year < author_first_publication[author]: # Keep earliest year \n",
    "            author_first_publication[author] = pub_year\n",
    "\n",
    "# Add info to nodes\n",
    "for node in tqdm(G.nodes()):\n",
    "    G.nodes[node][\"display_name\"] = \"Unknown\"\n",
    "    G.nodes[node][\"country\"] = \"Unknown\"\n",
    "    G.nodes[node][\"total_citations\"] = author_citation_counts.get(node, 0)\n",
    "    G.nodes[node][\"first_publication_year\"] = author_first_publication.get(node, None)\n",
    "\n",
    "\n",
    "graph_data = nx.node_link_data(G)\n",
    "\n",
    "with open(\"IC2S2_coauthorship_network.json\", \"w\") as json_file:\n",
    "    json.dump(graph_data, json_file, indent=4)\n",
    "\n",
    "print(\"\\nGraph saved successfully as 'IC2S2_coauthorship_network.json' ðŸŽ‰\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
