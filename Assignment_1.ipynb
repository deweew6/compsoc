{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_names():\n",
    "    program_url = \"https://ic2s2-2023.org/program\" # Program URL\n",
    "    chairs_url = \"https://ic2s2-2023.org/organization\" # Chairs URL\n",
    "    keynotes_url = \"https://ic2s2-2023.org/keynotes\" # Keynotes URL\n",
    "\n",
    "    # Scrape program page\n",
    "    program_response = requests.get(program_url)\n",
    "    program_soup = BeautifulSoup(program_response.text, 'html.parser')\n",
    "\n",
    "    # Extract and split names by comma and remove extra spaces\n",
    "    program_names = []\n",
    "    for tag in program_soup.find_all('i'):\n",
    "        names = tag.get_text(strip=True).split(',')  # Split by comma\n",
    "        clean_names = [name.strip('\" ').strip() for name in names]  # Remove extra spaces/quotes\n",
    "        program_names.extend(clean_names)  # Add to the final list\n",
    "\n",
    "    # Scrape chairs page\n",
    "    chairs_response = requests.get(chairs_url)\n",
    "    chairs_soup = BeautifulSoup(chairs_response.text, 'html.parser')\n",
    "    chairs_names = [a.get_text(strip=True) for a in chairs_soup.select(\"h3 a\")] # names are stored in <a> within <h3> tags\n",
    "\n",
    "    # Scape keynotes page\n",
    "    keynotes_response = requests.get(keynotes_url)\n",
    "    keynotes_soup = BeautifulSoup(keynotes_response.text, 'html.parser')\n",
    "    keynotes_names = [a.get_text(strip=True) for a in keynotes_soup.select(\"h3 a\")] # names are stored in <a> within <h3> tags again\n",
    "\n",
    "    # Returning names in dict \n",
    "    return {\n",
    "        \"program_names\": program_names,\n",
    "        \"chairs_names\": chairs_names,\n",
    "        \"keynotes_names\": keynotes_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = scrape_names() # Getting all names\n",
    "\n",
    "Elements_to_remove = [\"SAGE\", \"DIREC\", \"EPJ\", \"Esteban Moro (EPJkeynote)\"] # Found some errors in keynotes_names - we just remove them manually\n",
    "for element in Elements_to_remove:\n",
    "    names[\"keynotes_names\"].remove(element)\n",
    "\n",
    "names[\"keynotes_names\"].append(\"Esteban Moro\") # Add back Esteban Moro without the (EPJkeynote) part\n",
    "\n",
    "all_names = names[\"program_names\"] + names[\"chairs_names\"] + names[\"keynotes_names\"] # combining all names into a list\n",
    "\n",
    "all_names = list(set(all_names)) # This removes 578 duplicates\n",
    "\n",
    "# remove all names that contain the string Chair\n",
    "all_names = [name for name in all_names if \"Chair\" not in name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final list with names\n",
    "len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar names: [('Woo-sung Jung', 'Woo-Sung Jung'), ('Luca Verginer', 'luca verginer'), ('Zoe Rahwan', 'Zoe K. Rahwan'), ('Bin Wang', 'Qi Wang'), ('Katinka Den Nijs', 'Katinka den Nijs'), ('Keyu Zhu', 'Ke Zhou'), ('Sebastian Steffen', 'Sebastian Stier'), ('MÃ¡rton Karsai', 'Marton Karsai'), ('Nicholas Christakis', 'Nicholas A Christakis'), ('Anne C. Kroon', 'Anne Kroon'), ('Anne C. Kroon', 'Anne C Kroon'), ('Ana MarÃ­a Jaramillo', 'Ana Maria Jaramillo'), ('Sanja Scepanovic', 'Sanja Å Ä‡epanoviÄ‡'), ('Anne Kroon', 'Anne C Kroon'), ('Sam Wolken', 'Samuel Wolken'), ('Duncan J. Watts', 'Duncan J Watts'), ('Duncan J. Watts', 'Duncan Watts'), ('Francesco Silvestri', 'Francesco Pierri'), ('Rupert Kiddle', 'Rupert Tibor Kiddle'), ('Marcos Oliveira', 'Marcos A. Oliveira'), ('Zexun Chen', 'Ziwen Chen'), ('Michele Tizzani', 'Michele Tizzoni'), ('Duncan J Watts', 'Duncan Watts'), ('Federico Barrera-Lemarchand', 'Federico Barrera Lemarchand'), ('Matthew DeVerna', 'Matthew R DeVerna'), ('Kathyrn R Fair', 'Kathyrn Fair'), ('Diogo Pacheco', 'Diogo Pachecho'), ('Alessandro Flamini', 'Alessandro Flammini'), ('Luis E C Rocha', 'Luis M Rocha'), ('Leo Ferres', 'Leonardo Ferres'), ('David G. Rand', 'David Rand'), ('Sonja M Schmer-Galunder', 'Sonja M Schmer Galunder'), ('Francesco Barbieri', 'Francesco Pierri'), ('Stefan M. Herzog', 'Stefan Herzog'), ('Amy Smith', 'Abby Smith'), ('Fabio Carrella', 'Fabio Carella'), ('Xiao Zhang', 'Xiajie Zhang'), ('Hui Wang', 'Qi Wang'), ('Manoel Horta Ribeiro', 'Manoel Ribeiro'), ('Fabian Baumann', 'Fabian Braesemann'), ('Shintaro Ueki', 'Shintaro Sakai'), ('Mariano Gaston Beiro', 'Mariano GastÃ³n BeirÃ³'), ('Laura Maria Alessandretti', 'Laura Alessandretti'), ('Xindi Wang', 'Xinyi Wang'), ('Carlson Moses BÃ¼th', 'Carlson BÃ¼th'), ('Bedoor AlShebli', 'Bedoor Alshebli'), ('Lisette Espin-Noboa', 'Lisette Espin Noboa'), ('Sara Heydari', 'Babak Heydari'), ('Scott A Hale', 'Scott A. Hale'), ('Samuel Wolken', 'Samuel Woolley'), ('Pantelis P Analytis', 'Pantelis P. Analytis'), ('Pantelis P Analytis', 'Pantelis Analytis'), ('David Rothschild', 'David M Rothschild'), ('Huiyi Lv', 'Huiyi Lyu'), ('JosÃ© Javier Ramasco', 'Jose Javier Ramasco'), ('Feiyang Zhang', 'Yiyan Zhang'), ('Pantelis P. Analytis', 'Pantelis Analytis'), ('Yiyan Zhang', 'Yuan Zhang'), ('Carlos Rodriguez-Sickert', 'Carlos Rodriguez')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# < ---- > \n",
    "# Method to find highly similar names\n",
    "# Should work with defualt python libs\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def are_almost_similar(s1, s2, threshold=0.8):\n",
    "    return SequenceMatcher(None, s1, s2).ratio() >= threshold\n",
    "\n",
    "def find_similar_names(all_names, threshold=0.8):\n",
    "    similar_pairs = []\n",
    "    for i in range(len(all_names)):\n",
    "        for j in range(i + 1, len(all_names)):\n",
    "            if are_almost_similar(all_names[i], all_names[j], threshold):\n",
    "                similar_pairs.append((all_names[i], all_names[j]))\n",
    "    return similar_pairs\n",
    "\n",
    "# < ---- >\n",
    "\n",
    "# Find and print similar names\n",
    "similar_names = find_similar_names(all_names, threshold=0.8)\n",
    "print(f\"similar names: {similar_names}\")\n",
    "\n",
    "# In most cases the similar names are just the same name, but including the middle name or initial.\n",
    "# Lets keep the longest entry and remove the shorter one\n",
    "# Identify names to remove\n",
    "names_to_remove = {name2 if len(name1) > len(name2) else name1 for name1, name2 in similar_names}\n",
    "\n",
    "# Remove names\n",
    "all_names = [name for name in all_names if name not in names_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n"
     ]
    }
   ],
   "source": [
    "# Final list of names\n",
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scarping Names Explained\n",
    "\n",
    "### 1. Inspecting URLs\n",
    "First, I inspected all the URLs to determine the structure in which the names were stored.\n",
    "\n",
    "### 2. Collecting and Printing Names\n",
    "Then, I collected all the names and printed them from each URL to verify that the extraction worked correctly.\n",
    "\n",
    "### 3. Handling Errors\n",
    "During this process, I found some errors in `keynotes_names`, but only **four names** were incorrect, so I removed them manually.\n",
    "\n",
    "### 4. Removing Duplicates\n",
    "Next, I used Pythonâ€™s `set()` function to eliminate duplicates.\n",
    "\n",
    "### 5. Finding Similar Names\n",
    "I used a method to identify **slightly similar** names. After finding some matches, I decided to remove the shortest name of similair pairs. It was almost just a matter of includeing middlename / initials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pros and Cons of Custom-Made Data (Centola) vs. Ready-Made Data (Nicolaides)\n",
    "\n",
    "Centolaâ€™s study used custom-made data, which allowed him to control the structure of social networks. By creating artificial networks, he could adjust factors like how connected people were and how groups were formed. This control made it easier to see cause-and-effect relationships in behavior spread, making the results more reliable within the experiment. However, because the study took place in a simulated setting, it may not fully reflect how people behave in real life, limiting how well the findings apply outside the lab.\n",
    "\n",
    "Nicolaidesâ€™s study, on the other hand, used real-world data to observe how behaviors spread naturally in existing social networks. This makes the findings more realistic since they reflect actual human interactions. However, because the study was observational, there was less control over external factors. This means itâ€™s harder to prove that one specific factor caused the behavior to spread, as other influences might have played a role.\n",
    "\n",
    "### 2. Influence on Interpretation of Results\n",
    "\n",
    "The differences in how the two studies were conducted affect how we interpret their results. Centola used custom-made data and controlled network structures, which makes it easier to prove cause and effect in behavior spread. This is useful for building theories about how social influence works. However, since the study was done in an artificial setting, the results might not fully apply to real life.\n",
    "\n",
    "Nicolaides, on the other hand, studied real-world data, showing how behaviors actually spread in natural social networks. This makes the findings more realistic, but because the study wasnâ€™t controlled, itâ€™s harder to say for sure what caused the behaviors to spread. Other factors, like personal differences or outside influences, might have played a role. Overall, Centolaâ€™s study helps explain the theory behind social influence, while Nicolaidesâ€™s study shows how it works in practice, making them complementary to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting names to IDs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1444/1444 [02:28<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total authors not found: 258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving works in bulk: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:35<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique articles found: 43926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "#       Rate Limiter\n",
    "# -------------------------\n",
    "class RateLimiter:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate  # requests per second\n",
    "        self.min_interval = 1.0 / rate\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_time = 0.0\n",
    "\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = time.perf_counter()\n",
    "            elapsed = now - self.last_time\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_time = time.perf_counter()\n",
    "\n",
    "# Global rate limiter instance (10 requests per second)\n",
    "rate_limiter = RateLimiter(10)\n",
    "\n",
    "# -------------------------\n",
    "#  Convert names to IDs\n",
    "# -------------------------\n",
    "def convert_author_name_to_id(name):\n",
    "    query = urllib.parse.quote(name)\n",
    "    url = f\"https://api.openalex.org/authors?search={query}&filter=works_count:>5,works_count:<5000\"\n",
    "    try:\n",
    "        rate_limiter.wait()\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 429: # if this is trickered, we wait for 1 second and wait on rate limiter\n",
    "            time.sleep(1)\n",
    "            rate_limiter.wait()\n",
    "            response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e: # other errors, then we skip\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    if \"results\" not in data or len(data[\"results\"]) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take the first result\n",
    "    first_result = data[\"results\"][0]\n",
    "    return first_result[\"id\"]\n",
    "\n",
    "# -------------------------\n",
    "#  Get works by author IDs\n",
    "# -------------------------\n",
    "def get_works_by_authors_ids_bulk(author_ids):\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    authors_filter = \"author.id:\" + \"|\".join(author_ids)\n",
    "    filter_str = f\"{authors_filter},cited_by_count:>10,authors_count:<10\"\n",
    "    next_cursor = \"*\"\n",
    "    page_size = 200\n",
    "    all_works = []\n",
    "\n",
    "    while True:\n",
    "        url = (f\"{base_url}?filter={filter_str}\"\n",
    "               f\"&cursor={next_cursor}\"\n",
    "               f\"&per-page={page_size}\"\n",
    "               f\"&sort=publication_date:desc\")\n",
    "        try:\n",
    "            rate_limiter.wait()\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 429: # if this is trickered, we wait for 1 second\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        works_this_page = data.get(\"results\", [])\n",
    "        all_works.extend(works_this_page)\n",
    "        next_cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break\n",
    "\n",
    "    return all_works\n",
    "\n",
    "# -------------------------\n",
    "#       Main Logic\n",
    "# -------------------------\n",
    "\n",
    "# Convert all names to author IDs in parallel\n",
    "conversion_results = Parallel(n_jobs=10, backend=\"threading\")(\n",
    "    delayed(convert_author_name_to_id)(name) for name in tqdm(all_names, desc=\"Converting names to IDs\")\n",
    ")\n",
    "\n",
    "name_id_map = {name: author_id for name, author_id in zip(all_names, conversion_results) if author_id is not None}\n",
    "num_authors_not_found = len(all_names) - len(name_id_map)\n",
    "print(f\"\\nTotal authors not found: {num_authors_not_found}\")\n",
    "\n",
    "    \n",
    "author_ids = list(name_id_map.values()) # list containing all found author IDs\n",
    "\n",
    "\n",
    "def chunks(lst, n): # Splitting the list of author IDs into chunks of 25\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "author_id_chunks = list(chunks(author_ids, 25)) # calling the func\n",
    "\n",
    "# Get all works by author IDs in parallel\n",
    "works_lists = Parallel(n_jobs=10, backend=\"threading\")(\n",
    "    delayed(get_works_by_authors_ids_bulk)(chunk) for chunk in tqdm(author_id_chunks, desc=\"Retrieving works in bulk\")\n",
    ")\n",
    "\n",
    "# we collect only data for unique works\n",
    "all_works = [work for sublist in works_lists for work in sublist]\n",
    "unique_works = {}\n",
    "for w in all_works:\n",
    "    work_id = w.get(\"id\")\n",
    "    if work_id not in unique_works:\n",
    "        unique_works[work_id] = {\n",
    "            \"id\": work_id,\n",
    "            \"publication_year\": w.get(\"publication_year\"),\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "            \"author_ids\": [\n",
    "                auth[\"author\"][\"id\"]\n",
    "                for auth in w.get(\"authorships\", [])\n",
    "                if \"author\" in auth and \"id\" in auth[\"author\"]\n",
    "            ],\n",
    "            \"title\": w.get(\"title\"),\n",
    "            \"abstract_inverted_index\": w.get(\"abstract_inverted_index\"),\n",
    "        }\n",
    "\n",
    "print(f\"\\nTotal unique articles found: {len(unique_works)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#       Using the data from unique_works to create two dataframes - one for papers and one for abstracts\n",
    "# -------------------------\n",
    "papers_data = []\n",
    "abstracts_data = []\n",
    "\n",
    "for work_id, work in unique_works.items():\n",
    "    papers_data.append({\n",
    "        \"id\": work[\"id\"],\n",
    "        \"publication_year\": work[\"publication_year\"],\n",
    "        \"cited_by_count\": work[\"cited_by_count\"],\n",
    "        \"author_ids\": \",\".join(work[\"author_ids\"])  \n",
    "    })\n",
    "    abstracts_data.append({\n",
    "        \"id\": work[\"id\"],\n",
    "        \"title\": work[\"title\"],\n",
    "        \"abstract_inverted_index\": work[\"abstract_inverted_index\"]\n",
    "    })\n",
    "\n",
    "df_papers = pd.DataFrame(papers_data)\n",
    "df_abstracts = pd.DataFrame(abstracts_data)\n",
    "\n",
    "df_papers.to_csv(\"IC2S2_papers.csv\", index=False)\n",
    "df_abstracts.to_csv(\"IC2S2_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Research Articles using the OpenAlex API Explained\n",
    "\n",
    "### 1. Dataset Summary\n",
    "The IC2S2 papers dataset contains 43,737 works. These works were authored or co-authored by 1,179 (1444 - 265 not found) unique researchers.\n",
    "\n",
    "### 2. Efficiency in Code\n",
    "To improve efficiency, I implemented parallel processing using Joblibâ€™s Parallel to fetch author IDs and works concurrently, significantly reducing runtime. Additionally, I batched API requests, retrieving works for up to 25 authors at once, minimizing the number of queries sent. A rate limiter ensured compliance with OpenAlexâ€™s 10-requests-per-second limit, preventing failures due to excessive requests.\n",
    "\n",
    "### 3. Filtering Criteria and Dataset Relevance\n",
    "The dataset filters authors with at least 5 but fewer than 5,000 works, ensuring they are active but not overly prolific, which could introduce bias. Works are included only if they have at least 10 citations (indicating impact) and fewer than 10 authors (favoring individual contributions over large collaborations). These filters enhance dataset relevance by focusing on influential yet manageable research outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PART 4</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'a': 10, 'b': 20, 'c': 30}\n",
    "\n",
    "value_to_find = 20\n",
    "key = next(k for k, v in my_dict.items() if v == value_to_find)\n",
    "\n",
    "print(key)  # Output: 'b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275849/275849 [00:00<00:00, 5737751.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43926/43926 [00:00<00:00, 52943.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69432/69432 [00:00<00:00, 1026825.18it/s]\n",
      "/Users/benyla/miniconda3/lib/python3.10/site-packages/networkx/readwrite/json_graph/node_link.py:142: FutureWarning: \n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph saved successfully as 'IC2S2_coauthorship_network.json' ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "df_papers = pd.read_csv(\"IC2S2_papers.csv\")  # Load Papers Dataset\n",
    "\n",
    "id_name_map_reverse = {v: k for k, v in name_id_map.items()}\n",
    "\n",
    "# Create NetworkX Graph\n",
    "G = nx.Graph()\n",
    "collaboration_counts = Counter()\n",
    "\n",
    "for author_str in df_papers[\"author_ids\"]: # loop over each paper\n",
    "    authors = author_str.split(\",\") # split the authors (seperated by ,)\n",
    "    for author in authors:\n",
    "        G.add_node(author)  # Ensure every author is in the graph\n",
    "    for author1, author2 in itertools.combinations(authors, 2): # loop over each combination of authors\n",
    "        pair = tuple(sorted([author1, author2])) # sort the pair of authors to avoid duplicates\n",
    "        collaboration_counts[pair] += 1 # count the number of collaborations between the pair of authors\n",
    "\n",
    "G.add_weighted_edges_from([(a1, a2, count) for (a1, a2), count in tqdm(collaboration_counts.items())])\n",
    "\n",
    "\n",
    "\n",
    "# Compute citation counts and first publication year for each author\n",
    "author_citation_counts = Counter()\n",
    "author_first_publication = {}\n",
    "\n",
    "for _, row in tqdm(df_papers.iterrows(), total=len(df_papers)):\n",
    "    authors = row[\"author_ids\"].split(\",\")\n",
    "    cited_by = row[\"cited_by_count\"] # Number of citations for given paper\n",
    "    pub_year = row[\"publication_year\"] # Publication year of given paper\n",
    "\n",
    "    for author in authors:\n",
    "        author_citation_counts[author] += cited_by  # Sum up citations for each author\n",
    "        if author not in author_first_publication or pub_year < author_first_publication[author]: # Keep earliest year \n",
    "            author_first_publication[author] = pub_year\n",
    "\n",
    "# Add info to nodes\n",
    "for node in tqdm(G.nodes()):\n",
    "    G.nodes[node][\"display_name\"] = id_name_map_reverse.get(node, \"Unknown\")\n",
    "    G.nodes[node][\"country\"] = \"Unknown\"\n",
    "    G.nodes[node][\"total_citations\"] = author_citation_counts.get(node, 0)\n",
    "    G.nodes[node][\"first_publication_year\"] = author_first_publication.get(node, None)\n",
    "\n",
    "\n",
    "graph_data = nx.node_link_data(G)\n",
    "\n",
    "with open(\"IC2S2_coauthorship_network.json\", \"w\") as json_file:\n",
    "    json.dump(graph_data, json_file, indent=4)\n",
    "\n",
    "print(\"\\nGraph saved successfully as 'IC2S2_coauthorship_network.json' ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes (authors): 69432\n",
      "Total number of links (collaborations): 275849\n",
      "Network density: 0.000114\n",
      "Is the network fully connected? False\n",
      "Number of connected components: 189\n",
      "Number of isolated nodes: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "with open(\"IC2S2_coauthorship_network.json\", \"r\") as json_file:\n",
    "    graph_data = json.load(json_file)\n",
    "\n",
    "G = nx.node_link_graph(graph_data, edges=\"links\")\n",
    "\n",
    "# Number of nodes and edges\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "density = nx.density(G) # Network density\n",
    "\n",
    "is_connected = nx.is_connected(G) # Check if network is fully connected\n",
    "\n",
    "if not is_connected:\n",
    "    num_components = nx.number_connected_components(G) # Connecged components\n",
    "else:\n",
    "    num_components = 1  \n",
    "\n",
    "isolated_nodes = [n for n, degree in G.degree() if degree == 0] # Number of isolated nodes\n",
    "num_isolated = len(isolated_nodes)\n",
    "\n",
    "print(f\"Total number of nodes (authors): {num_nodes}\")\n",
    "print(f\"Total number of links (collaborations): {num_edges}\")\n",
    "print(f\"Network density: {density:.6f}\")\n",
    "print(f\"Is the network fully connected? {is_connected}\")\n",
    "print(f\"Number of connected components: {num_components}\")\n",
    "print(f\"Number of isolated nodes: {num_isolated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted Degree:\n",
      "  Average: 7.95\n",
      "  Median:  6.0\n",
      "  Mode:    [6]\n",
      "  Min:     0\n",
      "  Max:     2704\n",
      "\n",
      "Weighted Degree (Strength):\n",
      "  Average: 12.17\n",
      "  Median:  6.0\n",
      "  Mode:    [6]\n",
      "  Min:     0\n",
      "  Max:     4602\n",
      "\n",
      "Top 5 Authors by Degree:\n",
      "  Author: https://openalex.org/A5100460802 (Hui Wang), Degree: 2704\n",
      "  Author: https://openalex.org/A5100440745 (Zhen Xu), Degree: 1385\n",
      "  Author: https://openalex.org/A5059645286 (Robert West), Degree: 1280\n",
      "  Author: https://openalex.org/A5100381753 (Yang Wang), Degree: 1232\n",
      "  Author: https://openalex.org/A5100342298 (Yuwen Yu), Degree: 1066\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# ======= Degree Analysis =======\n",
    "degrees = [d for _, d in G.degree()]\n",
    "avg_degree = statistics.mean(degrees)\n",
    "med_degree = statistics.median(degrees)\n",
    "mode_degree = statistics.multimode(degrees)  # could be multiple modes\n",
    "min_degree = min(degrees)\n",
    "max_degree = max(degrees)\n",
    "\n",
    "weighted_degrees = [d for _, d in G.degree(weight=\"weight\")]\n",
    "avg_wdegree = statistics.mean(weighted_degrees)\n",
    "med_wdegree = statistics.median(weighted_degrees)\n",
    "mode_wdegree = statistics.multimode(weighted_degrees)\n",
    "min_wdegree = min(weighted_degrees)\n",
    "max_wdegree = max(weighted_degrees)\n",
    "\n",
    "print(\"Unweighted Degree:\")\n",
    "print(f\"  Average: {avg_degree:.2f}\")\n",
    "print(f\"  Median:  {med_degree}\")\n",
    "print(f\"  Mode:    {mode_degree}\")\n",
    "print(f\"  Min:     {min_degree}\")\n",
    "print(f\"  Max:     {max_degree}\")\n",
    "\n",
    "print(\"\\nWeighted Degree (Strength):\")\n",
    "print(f\"  Average: {avg_wdegree:.2f}\")\n",
    "print(f\"  Median:  {med_wdegree}\")\n",
    "print(f\"  Mode:    {mode_wdegree}\")\n",
    "print(f\"  Min:     {min_wdegree}\")\n",
    "print(f\"  Max:     {max_wdegree}\")\n",
    "\n",
    "\n",
    "# ======= Top Authors by Degree =======\n",
    "sorted_by_degree = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
    "top_5_authors = sorted_by_degree[:5]\n",
    "print(\"\\nTop 5 Authors by Degree:\")\n",
    "for author, deg in top_5_authors:\n",
    "    print(f\"  Author: {author} ({id_name_map_reverse[author]}), Degree: {deg}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
